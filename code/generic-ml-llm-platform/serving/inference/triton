# Triton Inference Server Configuration

# This directory will contain configurations and scripts for deploying models using Triton Inference Server.

# Sample configuration file for Triton Inference Server
# This is a basic example and should be customized based on your model requirements.

model_repository_config = {
    "model_repository": "/path/to/your/model/repository",
    "model_config": {
        "name": "your_model_name",
        "platform": "onnxruntime_onnx",
        "max_batch_size": 8,
        "input": [
            {
                "name": "input_tensor",
                "data_type": "TYPE_FP32",
                "dims": [1, 3, 224, 224]
            }
        ],
        "output": [
            {
                "name": "output_tensor",
                "data_type": "TYPE_FP32",
                "dims": [1, 1000]
            }
        ]
    }
}

# Save this configuration to a file named 'config.pbtxt' in the Triton model directory.
# Ensure to replace the placeholders with actual model details and paths.